import os
import json
import feedparser
import openai
from google.cloud import firestore
from datetime import datetime, timedelta, timezone
import hashlib
import logging
import functions_framework
from slack_sdk import WebhookClient
import requests
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')

# RSS ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆå¤šæ§˜ãªæƒ…å ±æºï¼‰
RSS_FEEDS = [
    # æ—¥æœ¬ã®ä¸»è¦ãƒ†ãƒƒã‚¯ã‚µã‚¤ãƒˆ
    {"url": "https://gigazine.net/news/rss_2.0/", "name": "GIGAZINE", "lang": "ja"},
    {"url": "https://www.itmedia.co.jp/rss/2.0/news_ai.xml", "name": "ITmedia AI", "lang": "ja"},
    {"url": "https://www.publickey1.jp/atom.xml", "name": "Publickey", "lang": "ja"},
    {"url": "https://japan.zdnet.com/rss/index.rdf", "name": "ZDNet Japan", "lang": "ja"},
    {"url": "https://ascii.jp/rss.xml", "name": "ASCII.jp", "lang": "ja"},
    {"url": "https://www.watch.impress.co.jp/data/rss/1.0/ipw/feed.rdf", "name": "Impress Watch", "lang": "ja"},
    {"url": "https://tech.nikkeibp.co.jp/rss/index.rdf", "name": "æ—¥çµŒxTECH", "lang": "ja"},
    {"url": "https://www.atmarkit.co.jp/rss/rss2dc.xml", "name": "@IT", "lang": "ja"},
    {"url": "https://codezine.jp/rss/new/20/index.xml", "name": "CodeZine", "lang": "ja"},
    {"url": "https://gihyo.jp/feed/rss2", "name": "æŠ€è¡“è©•è«–ç¤¾", "lang": "ja"},
    
    # æµ·å¤–ã®ä¸»è¦ãƒ†ãƒƒã‚¯ã‚µã‚¤ãƒˆ
    {"url": "https://feeds.feedburner.com/venturebeat/SZYF", "name": "VentureBeat", "lang": "en"},
    {"url": "https://techcrunch.com/feed/", "name": "TechCrunch", "lang": "en"},
    {"url": "https://feeds.feedburner.com/oreilly/radar", "name": "O'Reilly Radar", "lang": "en"},
    {"url": "https://rss.cnn.com/rss/cnn_tech.rss", "name": "CNN Tech", "lang": "en"},
    {"url": "https://www.theverge.com/rss/index.xml", "name": "The Verge", "lang": "en"},
    {"url": "https://feeds.arstechnica.com/arstechnica/index", "name": "Ars Technica", "lang": "en"},
    {"url": "https://www.wired.com/feed/rss", "name": "WIRED", "lang": "en"},
    {"url": "https://feeds.feedburner.com/Techradar", "name": "TechRadar", "lang": "en"},
    {"url": "https://www.engadget.com/rss.xml", "name": "Engadget", "lang": "en"},
    {"url": "https://feeds.feedburner.com/Mashable", "name": "Mashable", "lang": "en"},
    {"url": "https://www.zdnet.com/news/rss.xml", "name": "ZDNet", "lang": "en"},
    {"url": "https://feeds.feedburner.com/TechCrunchIT", "name": "TechCrunch IT", "lang": "en"},
    
    # AIå°‚é–€ã‚µã‚¤ãƒˆ
    {"url": "https://feeds.feedburner.com/aidaily", "name": "AI Daily", "lang": "en"},
    {"url": "https://www.artificialintelligence-news.com/feed/", "name": "AI News", "lang": "en"},
    {"url": "https://venturebeat.com/ai/feed/", "name": "VentureBeat AI", "lang": "en"},
    {"url": "https://www.unite.ai/feed/", "name": "Unite.AI", "lang": "en"},
    {"url": "https://towardsdatascience.com/feed", "name": "Towards Data Science", "lang": "en"},
    {"url": "https://machinelearningmastery.com/feed/", "name": "Machine Learning Mastery", "lang": "en"},
    {"url": "https://www.kdnuggets.com/feed", "name": "KDnuggets", "lang": "en"},
    {"url": "https://analyticsindiamag.com/feed/", "name": "Analytics India Magazine", "lang": "en"},
    {"url": "https://www.marktechpost.com/feed/", "name": "MarkTechPost", "lang": "en"},
    {"url": "https://syncedreview.com/feed/", "name": "Synced", "lang": "en"},
    
    # å­¦è¡“ãƒ»ç ”ç©¶ç³»
    {"url": "https://arxiv.org/rss/cs.AI", "name": "arXiv AI", "lang": "en"},
    {"url": "https://www.nature.com/subjects/machine-learning.rss", "name": "Nature ML", "lang": "en"},
    {"url": "https://arxiv.org/rss/cs.LG", "name": "arXiv Machine Learning", "lang": "en"},
    {"url": "https://arxiv.org/rss/cs.CL", "name": "arXiv NLP", "lang": "en"},
    {"url": "https://arxiv.org/rss/cs.CV", "name": "arXiv Computer Vision", "lang": "en"},
    {"url": "https://distill.pub/rss.xml", "name": "Distill", "lang": "en"},
    
    # ä¼æ¥­ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ç³»
    {"url": "https://openai.com/blog/rss.xml", "name": "OpenAI Blog", "lang": "en"},
    {"url": "https://blog.google/technology/ai/rss/", "name": "Google AI Blog", "lang": "en"},
    {"url": "https://engineering.fb.com/feed/", "name": "Meta Engineering", "lang": "en"},
    {"url": "https://blogs.microsoft.com/ai/feed/", "name": "Microsoft AI Blog", "lang": "en"},
    {"url": "https://aws.amazon.com/blogs/machine-learning/feed/", "name": "AWS ML Blog", "lang": "en"},
    {"url": "https://blog.tensorflow.org/feeds/posts/default", "name": "TensorFlow Blog", "lang": "en"},
    {"url": "https://pytorch.org/blog/feed.xml", "name": "PyTorch Blog", "lang": "en"},
    {"url": "https://huggingface.co/blog/feed.xml", "name": "Hugging Face Blog", "lang": "en"},
    {"url": "https://deepmind.com/blog/feed/basic/", "name": "DeepMind Blog", "lang": "en"},
    {"url": "https://blog.anthropic.com/rss.xml", "name": "Anthropic Blog", "lang": "en"},
    
    # é–‹ç™ºè€…ãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘
    {"url": "https://github.blog/feed/", "name": "GitHub Blog", "lang": "en"},
    {"url": "https://stackoverflow.blog/feed/", "name": "Stack Overflow Blog", "lang": "en"},
    {"url": "https://dev.to/feed", "name": "DEV Community", "lang": "en"},
    {"url": "https://hackernoon.com/feed", "name": "HackerNoon", "lang": "en"},
    {"url": "https://medium.com/feed/@towardsdatascience", "name": "Medium TDS", "lang": "en"},
    {"url": "https://www.infoq.com/feed/", "name": "InfoQ", "lang": "en"},
    
    # ãƒ“ã‚¸ãƒã‚¹ãƒ»æŠ•è³‡ç³»ï¼ˆAIé–¢é€£ï¼‰
    {"url": "https://www.cbinsights.com/research/feed/", "name": "CB Insights", "lang": "en"},
    {"url": "https://pitchbook.com/news/feed", "name": "PitchBook", "lang": "en"},
    {"url": "https://www.crunchbase.com/feed", "name": "Crunchbase News", "lang": "en"},
    
    # æ—¥æœ¬ã®AIãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ç³»
    {"url": "https://thebridge.jp/feed", "name": "THE BRIDGE", "lang": "ja"},
    {"url": "https://jp.techcrunch.com/feed/", "name": "TechCrunch Japan", "lang": "ja"},
    {"url": "https://www.startupdb.jp/feed", "name": "STARTUP DB", "lang": "ja"},
    {"url": "https://ainow.ai/feed/", "name": "AINOW", "lang": "ja"},
    {"url": "https://ledge.ai/feed/", "name": "Ledge.ai", "lang": "ja"}
]

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
openai.api_key = OPENAI_API_KEY
db = firestore.Client()

# Slacké€šçŸ¥ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
slack_client = WebhookClient(url=SLACK_WEBHOOK_URL) if SLACK_WEBHOOK_URL else None

def create_article_hash(url, title):
    """è¨˜äº‹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
    return hashlib.md5(f"{url}{title}".encode()).hexdigest()

def get_processed_articles():
    """Firestoreã‹ã‚‰å‡¦ç†æ¸ˆã¿è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    try:
        docs = db.collection('ai_articles').select(['hash']).stream()
        processed_hashes = set()
        
        for doc in docs:
            data = doc.to_dict()
            if 'hash' in data:
                processed_hashes.add(data['hash'])
        
        return processed_hashes
    except Exception as e:
        logger.error(f"Firestoreèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
        return set()

def is_ai_related_article(title, content):
    """GPTã‚’ä½¿ã£ã¦AIé–¢é€£ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": """ã‚ãªãŸã¯AIãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†é¡å°‚é–€å®¶ã§ã™ã€‚
                    è¨˜äº‹ãŒAIã€æ©Ÿæ¢°å­¦ç¿’ã€äººå·¥çŸ¥èƒ½ã€ChatGPTã€Claudeã€Geminiã€æ·±å±¤å­¦ç¿’ã€
                    è‡ªç„¶è¨€èªå‡¦ç†ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã€è‡ªå‹•åŒ–æŠ€è¡“ã€
                    ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã€MLOpsç­‰ã«é–¢é€£ã™ã‚‹ã€Œæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
                    
                    ã€å«ã‚ã‚‹ã€‘æ–°è£½å“ç™ºè¡¨ã€ä¼æ¥­ç™ºè¡¨ã€æŠ€è¡“é©æ–°ã€è²·åãƒ»ææºã€è¦åˆ¶ãƒ»æ”¿ç­–ã€ç ”ç©¶æˆæœ
                    ã€é™¤å¤–ã™ã‚‹ã€‘ç”¨èªè§£èª¬ã€ãƒã‚¦ãƒ„ãƒ¼è¨˜äº‹ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€åŸºæœ¬æ¦‚å¿µèª¬æ˜ã€éå»ã®æŒ¯ã‚Šè¿”ã‚Š
                    
                    åˆ¤å®šçµæœã¯ "YES" ã¾ãŸã¯ "NO" ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n\nå†…å®¹: {content[:1500]}"
                }
            ],
            max_tokens=10,
            temperature=0.1
        )
        
        result = response.choices[0].message.content.strip().upper()
        return result == "YES"
    except Exception as e:
        logger.error(f"AIé–¢é€£åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def extract_primary_sources(content, title):
    """è¨˜äº‹ã‹ã‚‰1æ¬¡æƒ…å ±ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """è¨˜äº‹ã‹ã‚‰1æ¬¡æƒ…å ±ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                    å„ªå…ˆé †ä½ï¼š
                    1. å…¬å¼ç™ºè¡¨ãƒ»ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹
                    2. ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆãƒ»ãƒ–ãƒ­ã‚°
                    3. GitHubãƒ»æŠ€è¡“æ–‡æ›¸
                    4. å…¬å¼Twitter/XæŠ•ç¨¿
                    5. ç ”ç©¶è«–æ–‡ãƒ»å­¦è¡“ã‚µã‚¤ãƒˆ
                    
                    ã¾ã¨ã‚ã‚µã‚¤ãƒˆã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚
                    è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ "ãªã—" ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n\nè¨˜äº‹å†…å®¹: {content[:2000]}"
                }
            ],
            max_tokens=200,
            temperature=0.1
        )
        
        result = response.choices[0].message.content.strip()
        return result if result != "ãªã—" else None
    except Exception as e:
        logger.error(f"1æ¬¡æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def summarize_with_openai(title, content, source_lang="en"):
    """OpenAI GPT-4o miniã§è¨˜äº‹ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã€é‡è¦åº¦ã‚‚è©•ä¾¡"""
    try:
        lang_instruction = "è¨˜äº‹ã¯è‹±èªã§ã™ãŒã€" if source_lang == "en" else ""
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": f"""ã‚ãªãŸã¯AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼è¨˜äº‹ã®è¦ç´„ãƒ»è©•ä¾¡ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆã§ã™ã€‚
                    {lang_instruction}ä»¥ä¸‹ã®ä½œæ¥­ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
                    
                    1. è¨˜äº‹ã‚’æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ã3-5æ–‡ã§è¦ç´„
                    2. AIåˆå¿ƒè€…ãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«ã¨ã£ã¦ã®é‡è¦åº¦ã‚’1-5ã§è©•ä¾¡
                    
                    é‡è¦åº¦åŸºæº–ï¼š
                    5: æ¥­ç•Œã‚’å¤‰ãˆã‚‹é©æ–°çš„ç™ºè¡¨ã€å¤§æ‰‹ä¼æ¥­ã®é‡è¦ç™ºè¡¨
                    4: æ³¨ç›®ã™ã¹ãæ–°æŠ€è¡“ã€é‡è¦ãªä¼æ¥­å‹•å‘  
                    3: èˆˆå‘³æ·±ã„é–‹ç™ºã€ä¸­ç¨‹åº¦ã®å½±éŸ¿
                    2: å°ã•ãªæ›´æ–°ã€é™å®šçš„ãªå½±éŸ¿
                    1: è»½å¾®ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å‚è€ƒç¨‹åº¦
                    
                    å‡ºåŠ›å½¢å¼ï¼š
                    è¦ç´„: [è¦ç´„æ–‡]
                    é‡è¦åº¦: [1-5ã®æ•°å€¤]"""
                },
                {
                    "role": "user",
                    "content": f"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}\n\nè¨˜äº‹å†…å®¹: {content[:3000]}"
                }
            ],
            max_tokens=500,
            temperature=0.3
        )
        
        result = response.choices[0].message.content.strip()
        
        # è¦ç´„ã¨é‡è¦åº¦ã‚’åˆ†é›¢
        lines = result.split('\n')
        summary = ""
        importance_score = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        for line in lines:
            if line.startswith('è¦ç´„:'):
                summary = line.replace('è¦ç´„:', '').strip()
            elif line.startswith('é‡è¦åº¦:'):
                try:
                    importance_score = int(line.replace('é‡è¦åº¦:', '').strip())
                    importance_score = max(1, min(5, importance_score))  # 1-5ã®ç¯„å›²ã«åˆ¶é™
                except ValueError:
                    importance_score = 3
        
        if not summary:
            summary = result  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç•°ãªã‚‹å ´åˆã¯å…¨ä½“ã‚’è¦ç´„ã¨ã—ã¦ä½¿ç”¨
        
        return summary, importance_score
        
    except Exception as e:
        logger.error(f"OpenAIè¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
        return "è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", 3

def save_to_firestore(title, url, summary, source, article_hash, source_lang, primary_source=None, importance_score=3):
    """è¦ç´„ã‚’Firestoreã«ä¿å­˜ï¼ˆæ‹¡å¼µãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼‰"""
    try:
        doc_ref = db.collection('ai_articles').document()
        doc_ref.set({
            'title': title,
            'url': url,
            'summary': summary,
            'date': datetime.now(timezone.utc),
            'source': source,
            'source_lang': source_lang,
            'hash': article_hash,
            'primary_source': primary_source,
            'processed': True,
            'used_in_summary': False,  # XæŠ•ç¨¿ã¾ã¨ã‚ã§ä½¿ç”¨æ¸ˆã¿ã‹ãƒ•ãƒ©ã‚°
            'importance_score': importance_score,     # é‡è¦åº¦ã‚¹ã‚³ã‚¢
            'created_at': firestore.SERVER_TIMESTAMP
        })
        
        logger.info(f"Firestoreã«ä¿å­˜å®Œäº†: {title}")
        return True
    except Exception as e:
        logger.error(f"Firestoreä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_recent_unused_articles(hours=24):
    """æœªä½¿ç”¨ã®æœ€è¿‘ã®è¨˜äº‹ã‚’é‡è¦åº¦é †ã§å–å¾—"""
    try:
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
        
        # ã¾ãšæœªä½¿ç”¨ã®è¨˜äº‹ã‚’å…¨ã¦å–å¾—ï¼ˆcreated_atãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„å ´åˆã‚‚è€ƒæ…®ï¼‰
        docs = db.collection('ai_articles').where(
            filter=firestore.FieldFilter('used_in_summary', '==', False)
        ).stream()
        
        articles = []
        for doc in docs:
            data = doc.to_dict()
            
            # created_atãŒãªã„å ´åˆã¯dateãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            article_time = data.get('created_at')
            if not article_time:
                article_time = data.get('date')
            
            # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„å ´åˆã¯å«ã‚ã‚‹ï¼‰
            if not article_time or article_time >= cutoff_time:
                articles.append({
                    'id': doc.id,
                    'title': data.get('title', ''),
                    'url': data.get('url', ''),
                    'summary': data.get('summary', ''),
                    'source': data.get('source', ''),
                    'primary_source': data.get('primary_source'),
                    'importance_score': data.get('importance_score', 3),
                    'created_at': article_time
                })
        
        # é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰ã€åŒã˜é‡è¦åº¦ãªã‚‰æ–°ã—ã„é †
        articles.sort(key=lambda x: (-x.get('importance_score', 3), -(x.get('created_at') or datetime.min).timestamp()))
        
        return articles
    except Exception as e:
        logger.error(f"æœªä½¿ç”¨è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def create_x_thread_summary(articles):
    """è¨˜äº‹ç¾¤ã‹ã‚‰XæŠ•ç¨¿ç”¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¾ã¨ã‚ã‚’ç”Ÿæˆï¼ˆâ˜…è©•ä¾¡ãƒ»å‚è€ƒãƒªãƒ³ã‚¯ä»˜ãï¼‰"""
    try:
        articles_text = "\n\n".join([
            f"ã€{article['source']}ã€‘{article['title']}\nè¦ç´„: {article['summary']}\né‡è¦åº¦: {article.get('importance_score', 3)}\nå‚è€ƒURL: {article['url']}\n1æ¬¡æƒ…å ±: {article.get('primary_source', 'ãªã—')}"
            for article in articles
        ])
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ã‚ãªãŸã¯AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å°‚é–€å®¶ã§ã™ã€‚
                    è¤‡æ•°ã®AIé–¢é€£è¨˜äº‹ã‹ã‚‰ã€Xï¼ˆTwitterï¼‰æŠ•ç¨¿ç”¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                    
                    å‡ºåŠ›å½¢å¼ï¼š
                    ã€ãƒ¡ã‚¤ãƒ³ãƒã‚¹ãƒˆã€‘
                    - ã‚­ãƒ£ãƒƒãƒãƒ¼ã§å°‚é–€çš„ãªå°å…¥æ–‡ï¼ˆ1-2è¡Œï¼‰
                    - é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’é‡è¦åº¦é †ï¼ˆâ˜…ã®å¤šã„é †ï¼‰ã§ç•ªå·ä»˜ããƒªã‚¹ãƒˆï¼ˆç°¡æ½”ãªè¦‹å‡ºã—ã®ã¿ï¼‰
                    - æœ€å¾Œã«ã€Œè¦‹é€ƒã›ãªã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’[N]ã¤ã‚¹ãƒ¬ãƒƒãƒ‰ã«ã¾ã¨ã‚ã¦ã„ã¾ã™ğŸ‘‡ğŸ§µã€
                    
                    ã€è©³ç´°ã‚¹ãƒ¬ãƒƒãƒ‰ã€‘
                    å„é …ç›®ã«ã¤ã„ã¦ï¼š
                    - ç•ªå·. è¦‹å‡ºã— â˜…â˜…â˜…â˜…â˜†ï¼ˆé‡è¦åº¦ã‚’â˜…ã§è¡¨ç¤ºï¼‰
                    - è©³ç´°èª¬æ˜ï¼ˆ2-3æ–‡ã€åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãï¼‰
                    - å‚è€ƒ: [å‚è€ƒURLã‚’ãã®ã¾ã¾è¨˜è¼‰]
                    - 1æ¬¡æƒ…å ±: [1æ¬¡æƒ…å ±ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°è¨˜è¼‰ã€ãªã‘ã‚Œã°çœç•¥]
                    
                    è¦ä»¶ï¼š
                    - é‡è¦åº¦ï¼ˆâ˜…ã®æ•°ï¼‰é †ã«ä¸¦ã¹ã‚‹
                    - åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã
                    - æŠ€è¡“çš„ã™ããšã€ãƒ“ã‚¸ãƒã‚¹çš„ä¾¡å€¤ã‚‚å«ã‚ã‚‹
                    - å„è©³ç´°ãƒã‚¹ãƒˆã¯ç‹¬ç«‹ã—ã¦ç†è§£ã§ãã‚‹ã‚ˆã†ã«
                    - å‚è€ƒURLã¯å¿…ãšå«ã‚ã‚‹"""
                },
                {
                    "role": "user",
                    "content": f"ä»¥ä¸‹ã®AIé–¢é€£è¨˜äº‹ã‹ã‚‰XæŠ•ç¨¿ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\n\n{articles_text}"
                }
            ],
            max_tokens=2500,
            temperature=0.4
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Xã¾ã¨ã‚ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def mark_articles_as_used(article_ids):
    """è¨˜äº‹ã‚’ä½¿ç”¨æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    try:
        for article_id in article_ids:
            doc_ref = db.collection('ai_articles').document(article_id)
            doc_ref.update({'used_in_summary': True})
        logger.info(f"{len(article_ids)}ä»¶ã®è¨˜äº‹ã‚’ä½¿ç”¨æ¸ˆã¿ã«ãƒãƒ¼ã‚¯")
    except Exception as e:
        logger.error(f"ä½¿ç”¨æ¸ˆã¿ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

def send_slack_notification_summary(x_summary, article_count):
    """XæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’Slackã«é€šçŸ¥"""
    if not slack_client or not x_summary:
        return
    
    try:
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"ğŸ¦ XæŠ•ç¨¿ç”¨AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆ{article_count}ä»¶ã‹ã‚‰ç”Ÿæˆï¼‰"
                }
            },
            {
                "type": "divider"
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"```{x_summary}```"
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": "ğŸ’¡ ãƒ¡ã‚¤ãƒ³ãƒã‚¹ãƒˆï¼‹è©³ç´°ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦Xï¼ˆTwitterï¼‰ã«æŠ•ç¨¿ã§ãã¾ã™"
                    }
                ]
            }
        ]
        
        response = slack_client.send(
            blocks=blocks,
            text=f"XæŠ•ç¨¿ç”¨AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆ{article_count}ä»¶ï¼‰"
        )
        
        logger.info(f"Slacké€šçŸ¥é€ä¿¡å®Œäº†: XæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰")
        return True
    except Exception as e:
        logger.error(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def send_slack_notification(articles):
    """å€‹åˆ¥è¨˜äº‹ã‚’Slackã«é€šçŸ¥ï¼ˆå¾“æ¥æ©Ÿèƒ½ï¼‰"""
    if not slack_client or not articles:
        return
    
    try:
        article_list = []
        for article in articles:
            article_list.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*<{article['url']}|{article['title']}>*\nğŸ“ {article['source']}\nğŸ’¡ {article['summary'][:100]}..."
                }
            })
        
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"ğŸ¤– AIé–¢é€£è¨˜äº‹ {len(articles)}ä»¶ã‚’æ¤œå‡º"
                }
            },
            {
                "type": "divider"
            }
        ]
        
        blocks.extend(article_list)
        
        response = slack_client.send(
            blocks=blocks,
            text=f"AIé–¢é€£è¨˜äº‹ {len(articles)}ä»¶ã‚’æ¤œå‡ºã—ã¾ã—ãŸ"
        )
        
        logger.info(f"Slacké€šçŸ¥é€ä¿¡å®Œäº†: {len(articles)}ä»¶")
        return True
    except Exception as e:
        logger.error(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_recent_articles():
    """æœ€è¿‘ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆç®¡ç†ç”¨ï¼‰"""
    try:
        docs = db.collection('ai_articles').order_by('created_at', direction=firestore.Query.DESCENDING).limit(20).stream()
        articles = []
        
        for doc in docs:
            data = doc.to_dict()
            articles.append({
                'title': data.get('title', ''),
                'url': data.get('url', ''),
                'source': data.get('source', ''),
                'summary': data.get('summary', ''),
                'source_lang': data.get('source_lang', ''),
                'primary_source': data.get('primary_source'),
                'used_in_summary': data.get('used_in_summary', False),
                'date': data.get('date', '').isoformat() if data.get('date') else ''
            })
        
        return articles
    except Exception as e:
        logger.error(f"è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_thread_history(days=7):
    """éå»ã®ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´ã‚’å–å¾—"""
    try:
        cutoff_time = datetime.now(timezone.utc) - timedelta(days=days)
        
        # ä½¿ç”¨æ¸ˆã¿è¨˜äº‹ã‚’å–å¾—ï¼ˆæœ€è¿‘ã®ã‚‚ã®ï¼‰
        docs = db.collection('ai_articles').where(
            filter=firestore.FieldFilter('used_in_summary', '==', True)
        ).where(
            filter=firestore.FieldFilter('created_at', '>=', cutoff_time)
        ).order_by('created_at', direction=firestore.Query.DESCENDING).stream()
        
        threads = {}
        for doc in docs:
            data = doc.to_dict()
            created_at = data.get('created_at')
            if created_at:
                # 6æ™‚é–“å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ç”Ÿæˆé–“éš”ã«åˆã‚ã›ã‚‹ï¼‰
                thread_key = created_at.replace(hour=(created_at.hour // 6) * 6, minute=0, second=0, microsecond=0)
                
                if thread_key not in threads:
                    threads[thread_key] = []
                
                threads[thread_key].append({
                    'id': doc.id,
                    'title': data.get('title', ''),
                    'url': data.get('url', ''),
                    'summary': data.get('summary', ''),
                    'source': data.get('source', ''),
                    'primary_source': data.get('primary_source'),
                    'importance_score': data.get('importance_score', 3),
                    'created_at': created_at
                })
        
        # å„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        for thread_key in threads:
            threads[thread_key].sort(key=lambda x: -x.get('importance_score', 3))
        
        return threads
    except Exception as e:
        logger.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def create_custom_thread_from_selection(selected_article_ids):
    """é¸æŠã•ã‚ŒãŸè¨˜äº‹IDã‹ã‚‰æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆ"""
    try:
        # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã‚’å–å¾—
        selected_articles = []
        for article_id in selected_article_ids:
            doc = db.collection('ai_articles').document(article_id).get()
            if doc.exists:
                data = doc.to_dict()
                selected_articles.append({
                    'id': doc.id,
                    'title': data.get('title', ''),
                    'url': data.get('url', ''),
                    'summary': data.get('summary', ''),
                    'source': data.get('source', ''),
                    'primary_source': data.get('primary_source'),
                    'importance_score': data.get('importance_score', 3),
                    'created_at': data.get('created_at')
                })
        
        if not selected_articles:
            return None
        
        # é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        selected_articles.sort(key=lambda x: -x.get('importance_score', 3))
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ç”Ÿæˆ
        thread_summary = create_x_thread_summary(selected_articles)
        
        return {
            'thread_summary': thread_summary,
            'articles_used': len(selected_articles),
            'articles': selected_articles
        }
    except Exception as e:
        logger.error(f"ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

@functions_framework.http
def rss_summarizer(request):
    """ãƒ¡ã‚¤ãƒ³ã®RSSè¦ç´„é–¢æ•°"""
    try:
        # GETãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æ©Ÿèƒ½ã‚’åˆ†å²
        action = request.args.get('action', 'collect')
        
        if action == 'list':
            articles = get_recent_articles()
            return json.dumps({
                'status': 'success',
                'articles': articles,
                'count': len(articles)
            }, ensure_ascii=False, indent=2), 200
        
        elif action == 'history':
            # éå»ã®ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´ã‚’å–å¾—
            days = int(request.args.get('days', 7))
            thread_history = get_thread_history(days)
            
            return json.dumps({
                'status': 'success',
                'action': 'history',
                'thread_history': {k.isoformat(): v for k, v in thread_history.items()},
                'thread_count': len(thread_history),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }, ensure_ascii=False, indent=2), 200
        
        elif action == 'custom':
            # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆ
            selected_ids = request.args.get('ids', '').split(',')
            selected_ids = [id.strip() for id in selected_ids if id.strip()]
            
            if not selected_ids:
                return json.dumps({
                    'status': 'error',
                    'message': 'idsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰'
                }, ensure_ascii=False), 400
            
            result = create_custom_thread_from_selection(selected_ids)
            if result:
                return json.dumps({
                    'status': 'success',
                    'action': 'custom_thread_created',
                    'articles_used': result['articles_used'],
                    'thread_summary': result['thread_summary'],
                    'articles': result['articles'],
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }, ensure_ascii=False, indent=2), 200
            else:
                return json.dumps({
                    'status': 'error',
                    'message': 'ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ'
                }, ensure_ascii=False), 500
        
        elif action == 'summary':
            # XæŠ•ç¨¿ç”¨ã¾ã¨ã‚ç”Ÿæˆ
            unused_articles = get_recent_unused_articles(24)
            if len(unused_articles) >= 3:  # æœ€ä½3ä»¶ä»¥ä¸Šã§å®Ÿè¡Œ
                x_summary = create_x_thread_summary(unused_articles)
                if x_summary:
                    # ä½¿ç”¨æ¸ˆã¿ãƒãƒ¼ã‚¯
                    article_ids = [article['id'] for article in unused_articles]
                    mark_articles_as_used(article_ids)
                    
                    # Slacké€šçŸ¥
                    send_slack_notification_summary(x_summary, len(unused_articles))
                    
                    return json.dumps({
                        'status': 'success',
                        'action': 'summary_created',
                        'articles_used': len(unused_articles),
                        'x_summary': x_summary,
                        'timestamp': datetime.now(timezone.utc).isoformat()
                    }, ensure_ascii=False, indent=2), 200
            
            return json.dumps({
                'status': 'success',
                'action': 'summary_skipped',
                'reason': f'è¨˜äº‹æ•°ä¸è¶³ï¼ˆ{len(unused_articles)}ä»¶ã€æœ€ä½3ä»¶å¿…è¦ï¼‰',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }, ensure_ascii=False, indent=2), 200
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¨˜äº‹åé›†
        logger.info("AIé–¢é€£RSSè¦ç´„å‡¦ç†ã‚’é–‹å§‹")
        processed_hashes = get_processed_articles()
        new_articles_count = 0
        processed_articles = []
        
        for feed_config in RSS_FEEDS:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]
            feed_lang = feed_config["lang"]
            
            logger.info(f"RSSå–å¾—ä¸­: {feed_name} ({feed_url})")
            
            try:
                feed = feedparser.parse(feed_url)
                
                if feed.bozo:
                    logger.warning(f"RSSè§£æè­¦å‘Š: {feed_name}")
                    continue
                
                # æœ€æ–°3è¨˜äº‹ã‚’å‡¦ç†
                for entry in feed.entries[:3]:
                    title = entry.title
                    url = entry.link
                    content = entry.get('summary', '') or entry.get('description', '')
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    article_hash = create_article_hash(url, title)
                    if article_hash in processed_hashes:
                        continue
                    
                    # AIé–¢é€£è¨˜äº‹ã‹ã©ã†ã‹åˆ¤å®š
                    if not is_ai_related_article(title, content):
                        logger.info(f"AIé–¢é€£å¤–è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: {title}")
                        continue
                    
                    logger.info(f"AIé–¢é€£è¨˜äº‹ã‚’å‡¦ç†ä¸­: {title}")
                    
                    # è¦ç´„ç”Ÿæˆ
                    summary, importance_score = summarize_with_openai(title, content, feed_lang)
                    
                    # 1æ¬¡æƒ…å ±æŠ½å‡º
                    primary_source = extract_primary_sources(content, title)
                    
                    # Firestoreã«ä¿å­˜
                    if save_to_firestore(title, url, summary, feed_name, article_hash, feed_lang, primary_source, importance_score):
                        new_articles_count += 1
                        processed_hashes.add(article_hash)
                        processed_articles.append({
                            'title': title,
                            'url': url,
                            'source': feed_name,
                            'summary': summary,
                            'primary_source': primary_source,
                            'importance_score': importance_score
                        })
                        
            except Exception as e:
                logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({feed_name}): {e}")
                continue
        
        # æ–°ã—ã„è¨˜äº‹ãŒã‚ã‚Œã°å€‹åˆ¥é€šçŸ¥
        if processed_articles:
            send_slack_notification(processed_articles)
        
        result = {
            'status': 'success',
            'action': 'collect',
            'new_ai_articles': new_articles_count,
            'articles': processed_articles,
            'total_feeds_checked': len(RSS_FEEDS),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        logger.info(f"å‡¦ç†å®Œäº†: {new_articles_count}ä»¶ã®AIé–¢é€£è¨˜äº‹")
        return json.dumps(result, ensure_ascii=False, indent=2), 200
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return json.dumps({'status': 'error', 'message': str(e)}, ensure_ascii=False), 500 